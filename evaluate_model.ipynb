{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/TonyYuanMD/Common_Distributions/blob/main/evaluate_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gePfdmzzSNsu"
      },
      "source": [
        "# Evaluate Fine-tuned Model on MATH Test Dataset\n",
        "\n",
        "This notebook evaluates the fine-tuned Qwen3 model downloaded from Aliyun on the MATH test dataset.\n",
        "\n",
        "## Setup\n",
        "- Model location: `model/` directory\n",
        "- Test dataset: `test_math.json`\n",
        "- Evaluation metric: Exact Match (EM) using SymPy normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WWCnu1-OSNs1",
        "outputId": "230d5169-b0a5-424c-b8c0-cdb683e730f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m43.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "# If you encounter version compatibility errors between transformers and peft,\n",
        "# you may need to install compatible versions. Try this cell first, and if it fails,\n",
        "# run the troubleshooting cell below.\n",
        "\n",
        "%pip install -q transformers accelerate peft sympy torch bitsandbytes\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EcAXZkf0SchS",
        "outputId": "994551d0-342a-4dac-af61-c507910e53e2"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft\n",
        "%pwd"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "id": "p9vHeP_iSnqF",
        "outputId": "ee2b351c-646d-46d8-aa25-8ea2fce7c469"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQk7SJuFSNs4"
      },
      "source": [
        "## Troubleshooting: Version Compatibility\n",
        "\n",
        "If you get an error like `ModuleNotFoundError: No module named 'transformers.modeling_layers'`,\n",
        "run the cell below to fix the version compatibility issue.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UfDlXiRBSNs4",
        "outputId": "f1f0791d-8c97-47be-9183-7d0935a24070"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.57.3-py3-none-any.whl.metadata (43 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.0/44.0 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: peft in /usr/local/lib/python3.12/dist-packages (0.18.0)\n",
            "Requirement already satisfied: accelerate in /usr/local/lib/python3.12/dist-packages (1.12.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from peft) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.12/dist-packages (from peft) (2.9.0+cu126)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.13.0->peft) (3.5.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.13.0->peft) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.3)\n",
            "Downloading transformers-4.57.3-py3-none-any.whl (12.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.0/12.0 MB\u001b[0m \u001b[31m143.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: transformers\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.57.2\n",
            "    Uninstalling transformers-4.57.2:\n",
            "      Successfully uninstalled transformers-4.57.2\n",
            "Successfully installed transformers-4.57.3\n",
            "If you see version errors, uncomment one of the options above and run this cell.\n"
          ]
        }
      ],
      "source": [
        "# TROUBLESHOOTING CELL - Run this if you get version compatibility errors\n",
        "# This installs compatible versions of transformers and peft\n",
        "\n",
        "# Option 1: Try upgrading to latest compatible versions\n",
        "%pip install --upgrade transformers peft accelerate\n",
        "\n",
        "# Option 2: Install specific compatible versions (recommended)\n",
        "# Uncomment the line below if Option 1 doesn't work:\n",
        "# %pip install \"transformers>=4.37.0,<4.52.0\" \"peft>=0.7.0\" accelerate sympy torch bitsandbytes\n",
        "\n",
        "# Option 3: Latest versions (may work, try if others fail)\n",
        "# %pip install \"transformers>=4.40.0\" \"peft>=0.10.0\" accelerate sympy torch bitsandbytes\n",
        "\n",
        "print(\"If you see version errors, uncomment one of the options above and run this cell.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OPGeGS1WSNs5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "import re\n",
        "import sympy\n",
        "import torch\n",
        "import random\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "# Try to import PeftModel (only needed if using LoRA adapter)\n",
        "# If import fails, we'll handle it when loading the adapter\n",
        "try:\n",
        "    from peft import PeftModel\n",
        "    PEFT_AVAILABLE = True\n",
        "except ImportError as e:\n",
        "    print(f\"Warning: peft not available or version incompatible: {e}\")\n",
        "    print(\"You can only use full models (not LoRA adapters) without peft.\")\n",
        "    PEFT_AVAILABLE = False\n",
        "    PeftModel = None\n",
        "\n",
        "# Configuration\n",
        "MODEL_DIR = \"model\"  # Path to the model DIRECTORY (folder containing model files, not the weights file itself)\n",
        "# MODEL_DIR should contain: config.json, tokenizer files, and either:\n",
        "#   - model.safetensors (full model), OR\n",
        "#   - adapter/ folder (LoRA adapter)\n",
        "TEST_DATA_PATH = \"test_math.json\"  # Path to test dataset\n",
        "USE_8BIT = True  # Use 8-bit quantization to save memory (requires bitsandbytes)\n",
        "MAX_NEW_TOKENS = 512\n",
        "NUM_SAMPLES = 50  # None means evaluate on all test samples, or set a number for quick testing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LIRKxUNfSNs6"
      },
      "source": [
        "## Load Evaluation Functions\n",
        "\n",
        "These functions are used to extract answers from model output and compare them.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "khYtu0BJSNs6",
        "outputId": "5fbf2012-fc34-4098-b6c0-e6e773d1a04d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation functions loaded!\n"
          ]
        }
      ],
      "source": [
        "def extract_boxed(latex_string):\n",
        "    \"\"\"Extract content from \\\\boxed{} in LaTeX string.\"\"\"\n",
        "    if not latex_string:\n",
        "        return None\n",
        "\n",
        "    match = re.search(r'\\\\boxed\\s*\\{', latex_string, re.IGNORECASE)\n",
        "    if not match:\n",
        "        return None\n",
        "\n",
        "    start_index = match.end()\n",
        "    brace_count = 1\n",
        "    content = []\n",
        "\n",
        "    for i in range(start_index, len(latex_string)):\n",
        "        char = latex_string[i]\n",
        "        if char == '{':\n",
        "            brace_count += 1\n",
        "            content.append(char)\n",
        "        elif char == '}':\n",
        "            brace_count -= 1\n",
        "            if brace_count == 0:\n",
        "                return \"\".join(content)\n",
        "            else:\n",
        "                content.append(char)\n",
        "        else:\n",
        "            content.append(char)\n",
        "    return None\n",
        "\n",
        "def normalize_sympy(s):\n",
        "    \"\"\"Normalize mathematical expression using sympy.\"\"\"\n",
        "    if not s:\n",
        "        return None\n",
        "    try:\n",
        "        return sympy.sympify(s)\n",
        "    except (sympy.SympifyError, TypeError):\n",
        "        return None\n",
        "\n",
        "print(\"Evaluation functions loaded!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# extract_boxed(\"\\\\boxed{\\\\frac{\\\\boldsymbol{x}}{1}}\")\n",
        "normalize_sympy(\"1/2\") == normalize_sympy(\"0.5\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHZRbNCTsWzE",
        "outputId": "d21193b7-ba57-466c-ea9b-9c0a3a4bb350"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHwhLC1QSNs7"
      },
      "source": [
        "## Load Model and Tokenizer\n",
        "\n",
        "The notebook will automatically detect if the model is a full model or a LoRA adapter.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HScW6QaPSNs8",
        "outputId": "8d053b8b-e822-4dad-b5bf-e29ae0214869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model directory: /content/drive/MyDrive/Colab_Notebooks/CSE595_Proj/Aliyun/sft/model\n",
            "Has full model: True\n",
            "Has adapter: True\n",
            "Tokenizer loaded: Qwen2TokenizerFast\n",
            "Vocab size: 151643\n"
          ]
        }
      ],
      "source": [
        "# Check if we have a full model or LoRA adapter\n",
        "model_path = Path(MODEL_DIR)\n",
        "has_full_model = (model_path / \"model.safetensors\").exists() or (model_path / \"pytorch_model.bin\").exists()\n",
        "has_adapter = (model_path / \"adapter\" / \"adapter_config.json\").exists()\n",
        "\n",
        "print(f\"Model directory: {model_path.absolute()}\")\n",
        "print(f\"Has full model: {has_full_model}\")\n",
        "print(f\"Has adapter: {has_adapter}\")\n",
        "\n",
        "if not has_full_model and not has_adapter:\n",
        "    raise ValueError(f\"No model found in {MODEL_DIR}. Please check the path.\")\n",
        "\n",
        "# Check if adapter requires peft\n",
        "if has_adapter and not PEFT_AVAILABLE:\n",
        "    raise ValueError(\n",
        "        \"LoRA adapter detected but peft is not available or incompatible.\\n\"\n",
        "        \"Please fix the version compatibility issue:\\n\"\n",
        "        \"Option 1: Upgrade packages: pip install --upgrade transformers peft\\n\"\n",
        "        \"Option 2: Install compatible versions: pip install 'transformers>=4.37.0,<4.52.0' 'peft>=0.7.0'\\n\"\n",
        "        \"Option 3: Use a full model instead of a LoRA adapter\"\n",
        "    )\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\n",
        "    MODEL_DIR,\n",
        "    trust_remote_code=True\n",
        ")\n",
        "\n",
        "# Set pad token if not set\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "print(f\"Tokenizer loaded: {tokenizer.__class__.__name__}\")\n",
        "print(f\"Vocab size: {tokenizer.vocab_size}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocess Test Dataset (Extract Ground Truth Answers)\n",
        "\n",
        "Preprocess the test dataset to extract and normalize ground truth boxed answers in advance.\n",
        "This speeds up evaluation since we don't need to extract answers for each example during evaluation.\n"
      ],
      "metadata": {
        "id": "QaCNebOnsfIK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Preprocess test dataset: extract and normalize ground truth answers\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "\n",
        "PREPROCESSED_TEST_PATH = \"test_math_preprocessed.json\"  # Output path for preprocessed data\n",
        "\n",
        "# Check if preprocessed file already exists\n",
        "if os.path.exists(PREPROCESSED_TEST_PATH):\n",
        "    print(f\"Preprocessed file {PREPROCESSED_TEST_PATH} already exists.\")\n",
        "    print(\"Loading preprocessed data...\")\n",
        "    with open(PREPROCESSED_TEST_PATH, 'r', encoding='utf-8') as f:\n",
        "        test_data = json.load(f)\n",
        "    print(f\"Loaded {len(test_data)} preprocessed examples\")\n",
        "\n",
        "    # Show an example to verify format\n",
        "    if len(test_data) > 0:\n",
        "        example = test_data[0]\n",
        "        print(\"\\nExample preprocessed item:\")\n",
        "        print(f\"  Instruction: {example['instruction'][:100]}...\")\n",
        "        print(f\"  Gold boxed answer (raw): {example.get('gold_answer_str', 'N/A')}\")\n",
        "        print(f\"  Gold answer (normalized): {example.get('gold_answer_sympy', 'N/A')}\")\n",
        "else:\n",
        "    print(\"Preprocessing test dataset...\")\n",
        "    print(\"This may take a few minutes for large datasets.\")\n",
        "\n",
        "    # Load original test data\n",
        "    with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "        original_test_data = json.load(f)\n",
        "\n",
        "    print(f\"Processing {len(original_test_data)} examples...\")\n",
        "\n",
        "    # Preprocess each example\n",
        "    preprocessed_data = []\n",
        "    for item in tqdm(original_test_data, desc=\"Preprocessing\"):\n",
        "        instruction = item[\"instruction\"]\n",
        "        gold_output = item[\"output\"]\n",
        "\n",
        "        # Extract boxed answer from gold output\n",
        "        gold_ans_str = extract_boxed(gold_output)\n",
        "\n",
        "        # Normalize using sympy\n",
        "        gold_ans_sym = normalize_sympy(gold_ans_str)\n",
        "\n",
        "        # Store normalized answer as string for comparison\n",
        "        gold_ans_sym_str = str(gold_ans_sym) if gold_ans_sym is not None else None\n",
        "\n",
        "        # Create preprocessed item\n",
        "        preprocessed_item = {\n",
        "            \"instruction\": instruction,\n",
        "            \"output\": gold_output,  # Keep original output for reference\n",
        "            \"gold_answer_str\": gold_ans_str,  # Extracted boxed answer (raw string)\n",
        "            \"gold_answer_sympy\": gold_ans_sym_str,  # Normalized answer (as string for JSON)\n",
        "            \"gold_answer_sympy_obj\": None  # Will be None, we'll reconstruct from string\n",
        "        }\n",
        "\n",
        "        preprocessed_data.append(preprocessed_item)\n",
        "\n",
        "    # Save preprocessed data\n",
        "    with open(PREPROCESSED_TEST_PATH, 'w', encoding='utf-8') as f:\n",
        "        json.dump(preprocessed_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "    print(f\"✓ Preprocessing complete! Saved to {PREPROCESSED_TEST_PATH}\")\n",
        "    print(f\"  Processed {len(preprocessed_data)} examples\")\n",
        "\n",
        "    # Statistics\n",
        "    valid_answers = sum(1 for item in preprocessed_data if item[\"gold_answer_sympy\"] is not None)\n",
        "    print(f\"  Valid boxed answers extracted: {valid_answers}/{len(preprocessed_data)}\")\n",
        "\n",
        "    # Load the preprocessed data\n",
        "    test_data = preprocessed_data\n",
        "\n",
        "print(\"\\nPreprocessing ready!\")\n"
      ],
      "metadata": {
        "id": "rL4Em7VKse2Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p8JAINWGSNs8",
        "outputId": "e6c26011-0ffc-4338-b0fa-2671f7936606"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading base model from: /tmp/input_model/\n",
            "Base model path not found, using model directory: model\n",
            "Loading LoRA adapter from: model/adapter\n",
            "LoRA adapter loaded successfully!\n",
            "Model device: cuda:0\n",
            "Model dtype: torch.float16\n"
          ]
        }
      ],
      "source": [
        "# Load model based on type\n",
        "if has_adapter and PEFT_AVAILABLE:\n",
        "    # Load base model first (need to check config for base model path)\n",
        "    adapter_config_path = model_path / \"adapter\" / \"adapter_config.json\"\n",
        "    with open(adapter_config_path, 'r') as f:\n",
        "        adapter_config = json.load(f)\n",
        "\n",
        "    # Note: If base_model_name_or_path is a local path that doesn't exist,\n",
        "    # you may need to manually specify the base model name\n",
        "    base_model_path = adapter_config.get(\"base_model_name_or_path\", \"\")\n",
        "\n",
        "    print(f\"Loading base model from: {base_model_path}\")\n",
        "\n",
        "    # Try to load base model - if path doesn't exist, we'll try loading from MODEL_DIR\n",
        "    if os.path.exists(base_model_path):\n",
        "        base_model_path_to_use = base_model_path\n",
        "    else:\n",
        "        # Assume the full model is in MODEL_DIR (if it exists)\n",
        "        if has_full_model:\n",
        "            base_model_path_to_use = str(model_path)\n",
        "            print(f\"Base model path not found, using model directory: {base_model_path_to_use}\")\n",
        "        else:\n",
        "            # If no base model found, we need the HuggingFace model name\n",
        "            # This should be provided by the user or detected from config\n",
        "            raise ValueError(f\"Cannot find base model. Please check the adapter config.\")\n",
        "\n",
        "    # Import BitsAndBytesConfig for quantization (if available)\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quantization_config = None\n",
        "        if USE_8BIT:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=6.0,\n",
        "            )\n",
        "    except ImportError:\n",
        "        print(\"Warning: bitsandbytes not available. Setting USE_8BIT=False\")\n",
        "        USE_8BIT = False\n",
        "        quantization_config = None\n",
        "\n",
        "    # Load base model\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_path_to_use if os.path.exists(base_model_path_to_use) else MODEL_DIR,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "\n",
        "    # Load adapter\n",
        "    adapter_path = model_path / \"adapter\"\n",
        "    print(f\"Loading LoRA adapter from: {adapter_path}\")\n",
        "    model = PeftModel.from_pretrained(base_model, str(adapter_path))\n",
        "    print(\"LoRA adapter loaded successfully!\")\n",
        "\n",
        "else:\n",
        "    # Load full model\n",
        "    # Import BitsAndBytesConfig for quantization (if available)\n",
        "    try:\n",
        "        from transformers import BitsAndBytesConfig\n",
        "        quantization_config = None\n",
        "        if USE_8BIT:\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_8bit=True,\n",
        "                llm_int8_threshold=6.0,\n",
        "            )\n",
        "    except ImportError:\n",
        "        print(\"Warning: bitsandbytes not available. Setting USE_8BIT=False\")\n",
        "        USE_8BIT = False\n",
        "        quantization_config = None\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_DIR,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quantization_config,\n",
        "        torch_dtype=torch.float16 if not USE_8BIT else None,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    print(\"Full model loaded successfully!\")\n",
        "\n",
        "model.eval()\n",
        "print(f\"Model device: {next(model.parameters()).device}\")\n",
        "print(f\"Model dtype: {next(model.parameters()).dtype}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvH16QNRSNs9"
      },
      "source": [
        "## Load Test Dataset\n",
        "\n",
        "Load the test dataset in JSON format (instruction/output pairs).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qPjBv0sSNs-",
        "outputId": "46580712-4d76-4b74-c4be-6db0e523e63b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 5000 test examples\n",
            "Randomly selected 50 samples for evaluation\n",
            "\n",
            "Example test item:\n",
            "{\n",
            "  \"instruction\": \"You are a math assistant. Solve the problem step by step, explain your reasoning, and box the final answer using \\\\boxed{}.\\n\\nIf $a$ and $b$ are real numbers, $a^2b^3=\\\\frac{32}{27}$, and $\\\\frac{a}{b^3}=\\\\frac{27}{4}$, what is $a+b$?\",\n",
            "  \"output\": \"Rearranging the second equation, we have that $b^3=\\\\frac{4}{27}a$. If we substitute this into the original equation, we get $\\\\frac{4}{27}a^3=\\\\frac{32}{27}$; after multiplying each side by $\\\\frac{27}{4}$ and taking the cube root, we see that $a=2$. Substituting $a$ into the first equation, we get that $b^3=\\\\frac{8}{27}$ or $b=\\\\frac23$. Thus, $a+b=2+\\\\frac23=\\\\boxed{\\\\frac83}$.\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "# Load test dataset\n",
        "with open(TEST_DATA_PATH, 'r', encoding='utf-8') as f:\n",
        "    test_data = json.load(f)\n",
        "\n",
        "print(f\"Loaded {len(test_data)} test examples\")\n",
        "\n",
        "# Limit number of samples if specified and draw randomly\n",
        "if NUM_SAMPLES is not None:\n",
        "    if NUM_SAMPLES <= len(test_data):\n",
        "        random.seed(42) # For reproducibility\n",
        "        test_data = random.sample(test_data, NUM_SAMPLES)\n",
        "        print(f\"Randomly selected {NUM_SAMPLES} samples for evaluation\")\n",
        "    else:\n",
        "        print(f\"NUM_SAMPLES ({NUM_SAMPLES}) is greater than total examples ({len(test_data)}). Using all examples.\")\n",
        "\n",
        "# Show an example\n",
        "print(\"\\nExample test item:\")\n",
        "print(json.dumps(test_data[0], indent=2, ensure_ascii=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-7thHEaSNs-"
      },
      "source": [
        "## Evaluation Function\n",
        "\n",
        "This function evaluates the model on the test dataset and computes exact match accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLEnkEHESNs_",
        "outputId": "4f95f528-7934-4121-ed04-d3ad78c7f452"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Evaluation function defined!\n"
          ]
        }
      ],
      "source": [
        "def evaluate_model(model, tokenizer, test_data, max_new_tokens=512, verbose=True):\n",
        "    \"\"\"\n",
        "    Evaluate model on test dataset.\n",
        "    Returns: exact_match_score, detailed_results\n",
        "    \"\"\"\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    results = []\n",
        "\n",
        "    # Extract system prompt and user question from instruction\n",
        "    # Format: \"You are a math assistant...\\\\n\\\\n<problem>\"\n",
        "    SYSTEM_PROMPT = \"You are a math assistant. Solve the problem step by step, explain your reasoning, and box the final answer using \\\\boxed{}.\"\n",
        "\n",
        "    for idx, item in enumerate(tqdm(test_data, desc=\"Evaluating\")):\n",
        "        instruction = item[\"instruction\"]\n",
        "        gold_output = item[\"output\"]\n",
        "\n",
        "        # Parse instruction to get system prompt and problem\n",
        "        # The instruction format is: \"You are a math assistant...\\n\\n<problem>\"\n",
        "        # Try both actual newlines and escaped newlines\n",
        "        if \"\\n\\n\" in instruction:\n",
        "            parts = instruction.split(\"\\n\\n\", 1)\n",
        "            system_msg = parts[0]\n",
        "            problem = parts[1]\n",
        "        elif \"\\\\n\\\\n\" in instruction:\n",
        "            parts = instruction.split(\"\\\\n\\\\n\", 1)\n",
        "            system_msg = parts[0]\n",
        "            problem = parts[1]\n",
        "        else:\n",
        "            # If no separator, use the whole instruction as problem\n",
        "            system_msg = SYSTEM_PROMPT\n",
        "            problem = instruction\n",
        "\n",
        "        # Extract gold answer\n",
        "        gold_ans_str = extract_boxed(gold_output)\n",
        "        gold_ans_sym = normalize_sympy(gold_ans_str)\n",
        "\n",
        "        # Construct prompt in Qwen3 format\n",
        "        # Format: <|im_start|>system\\n<system_prompt><|im_end|>\\n<|im_start|>user\\n<problem><|im_end|>\\n<|im_start|>assistant\\n\n",
        "        prompt = f\"<|im_start|>system\\n{system_msg}<|im_end|>\\n<|im_start|>user\\n{problem}<|im_end|>\\n<|im_start|>assistant\\n\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=2048).to(model.device)\n",
        "        input_length = inputs[\"input_ids\"].shape[1]\n",
        "\n",
        "        # Generate\n",
        "        with torch.no_grad():\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=0.7,\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id\n",
        "            )\n",
        "\n",
        "        # Decode only the newly generated tokens\n",
        "        generated_tokens = outputs[0][input_length:]\n",
        "        pred_text = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n",
        "\n",
        "        # Extract predicted answer\n",
        "        pred_ans_str = extract_boxed(pred_text)\n",
        "        pred_ans_sym = normalize_sympy(pred_ans_str)\n",
        "\n",
        "        # Check if correct\n",
        "        is_correct = False\n",
        "        if pred_ans_sym is not None and gold_ans_sym is not None:\n",
        "            is_correct = (pred_ans_sym == gold_ans_sym)\n",
        "        elif (pred_ans_str == \"\" or pred_ans_str is None) and (gold_ans_str == \"\" or gold_ans_str is None):\n",
        "            is_correct = True\n",
        "\n",
        "        if is_correct:\n",
        "            correct += 1\n",
        "        total += 1\n",
        "\n",
        "        results.append({\n",
        "            \"problem\": problem[:100] + \"...\" if len(problem) > 100 else problem,\n",
        "            \"predicted\": pred_ans_str,\n",
        "            \"gold\": gold_ans_str,\n",
        "            \"predicted_full\": pred_text[:200] + \"...\" if len(pred_text) > 200 else pred_text,\n",
        "            \"correct\": is_correct\n",
        "        })\n",
        "\n",
        "        if verbose and idx < 5:  # Show first 5 examples\n",
        "            print(f\"\\n--- Example {idx + 1} ---\")\n",
        "            print(f\"Problem: {problem[:150]}...\")\n",
        "            print(f\"Predicted answer: {pred_ans_str}\")\n",
        "            print(f\"Gold answer: {gold_ans_str}\")\n",
        "            print(f\"Correct: {is_correct}\")\n",
        "            print(f\"Generated text (first 200 chars): {pred_text[:200]}...\")\n",
        "\n",
        "    exact_match = correct / total if total > 0 else 0.0\n",
        "    return exact_match, results\n",
        "\n",
        "print(\"Evaluation function defined!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTYgA8dBSNs_"
      },
      "source": [
        "## Run Evaluation\n",
        "\n",
        "Evaluate the model on the test dataset and compute exact match accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eJeeZh8vSNtA",
        "outputId": "ad6b38b8-38dd-4193-863d-4cf4d7e0ae41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting evaluation...\n",
            "Total test examples: 50\n",
            "Max new tokens: 512\n",
            "------------------------------------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating:   2%|▏         | 1/50 [01:40<1:22:17, 100.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 1 ---\n",
            "Problem: If $a$ and $b$ are real numbers, $a^2b^3=\\frac{32}{27}$, and $\\frac{a}{b^3}=\\frac{27}{4}$, what is $a+b$?...\n",
            "Predicted answer: \\frac{219}{28}\n",
            "Gold answer: \\frac83\n",
            "Correct: False\n",
            "Generated text (first 200 chars): If we set $a=\\frac{27}{4}$ and $b=\\frac{12}{7}$, then $a^2b^3=\\frac{32}{27}$ and $\\frac{a}{b^3}=\\frac{27}{4}$, so $a+b=\\frac{27}{4}+\\frac{12}{7}=\\frac{219}{28}$.  (Note that we have chosen values for ...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   4%|▍         | 2/50 [01:57<41:05, 51.37s/it]   "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 2 ---\n",
            "Problem: If $f(x) = x + 1$ and $g(x) = x^2 + 3$, what is the value of $f(g(2))$?...\n",
            "Predicted answer: 8\n",
            "Gold answer: 8\n",
            "Correct: True\n",
            "Generated text (first 200 chars): We proceed as follows: \\begin{align*}\n",
            "f(g(2)) &= f(2^2 + 3) \\\\\n",
            "&= f(7) \\\\\n",
            "&= 7 + 1 \\\\\n",
            "&= \\boxed{8}.\n",
            "\\end{align*} (Note that we don't actually need to know what $g(2)$ is, because we can just plug it i...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   6%|▌         | 3/50 [02:53<42:02, 53.67s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 3 ---\n",
            "Problem: For some real numbers $a$ and $b$, the equation \\[\n",
            "8x^3 + 4ax^2 + 2bx + a = 0\n",
            "\\]has three distinct positive roots. If the sum of the base-2 logarithms...\n",
            "Predicted answer: -8\n",
            "Gold answer: -256\n",
            "Correct: False\n",
            "Generated text (first 200 chars): From the given information, we must have \\[\n",
            "\\log_2 r_1 + \\log_2 r_2 + \\log_2 r_3 = 5.\n",
            "\\]From the properties of logarithms, we can write this as \\[\\log_2 (r_1 r_2 r_3) = 5.\\]So, \\[r_1 r_2 r_3 = 2^5 = 3...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:   8%|▊         | 4/50 [04:31<54:23, 70.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 4 ---\n",
            "Problem: Let $ABCD$ be a regular tetrahedron with side length 2. The plane parallel to edges $AB$ and $CD$ and lying halfway between them cuts $ABCD$ into two ...\n",
            "Predicted answer: None\n",
            "Gold answer: 1+2\\sqrt{3}\n",
            "Correct: False\n",
            "Generated text (first 200 chars): Let $P$ and $Q$ be the centers of the faces $BCD$ and $ACD,$ respectively, and let $R$ be the center of the face $ABC.$  Then $\\triangle BPC$ is equilateral with side length 2, so $BP = 2.$  Similarly...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rEvaluating:  10%|█         | 5/50 [05:58<57:32, 76.72s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Example 5 ---\n",
            "Problem: Compute $\\sin 30^\\circ$....\n",
            "Predicted answer: \\frac{1}{2}\n",
            "Gold answer: \\frac{1}{2}\n",
            "Correct: False\n",
            "Generated text (first 200 chars): We draw a right triangle with one angle equal to $30^\\circ$.  Then, by drawing an altitude on the hypotenuse, we can create a $30^\\circ-60^\\circ-90^\\circ$ triangle.  For a $30^\\circ-60^\\circ-90^\\circ$...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 50/50 [1:10:59<00:00, 85.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "EVALUATION RESULTS\n",
            "============================================================\n",
            "Exact Match Score: 0.1800 (18.00%)\n",
            "Correct: 9\n",
            "Total: 50\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting evaluation...\")\n",
        "print(f\"Total test examples: {len(test_data)}\")\n",
        "print(f\"Max new tokens: {MAX_NEW_TOKENS}\")\n",
        "print(\"-\" * 60)\n",
        "\n",
        "exact_match_score, detailed_results = evaluate_model(\n",
        "    model,\n",
        "    tokenizer,\n",
        "    test_data,\n",
        "    max_new_tokens=MAX_NEW_TOKENS,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(f\"EVALUATION RESULTS\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"Exact Match Score: {exact_match_score:.4f} ({exact_match_score*100:.2f}%)\")\n",
        "print(f\"Correct: {sum(r['correct'] for r in detailed_results)}\")\n",
        "print(f\"Total: {len(detailed_results)}\")\n",
        "print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z2mk3L1XSNtA"
      },
      "source": [
        "## Save Results\n",
        "\n",
        "Save the evaluation results to a file for later analysis.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIsw0Ms3SNtA",
        "outputId": "198212c7-e39d-404e-e6bc-52f379a796e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Results saved to evaluation_results.json\n",
            "\n",
            "Statistics:\n",
            "  Accuracy: 18.00%\n",
            "  Correct: 9\n",
            "  Incorrect: 41\n"
          ]
        }
      ],
      "source": [
        "# Save results to JSON file\n",
        "results_file = \"evaluation_results.json\"\n",
        "results_summary = {\n",
        "    \"exact_match_score\": exact_match_score,\n",
        "    \"total_examples\": len(detailed_results),\n",
        "    \"correct\": sum(r['correct'] for r in detailed_results),\n",
        "    \"detailed_results\": detailed_results\n",
        "}\n",
        "\n",
        "with open(results_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(results_summary, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"Results saved to {results_file}\")\n",
        "\n",
        "# Show some statistics\n",
        "print(f\"\\nStatistics:\")\n",
        "print(f\"  Accuracy: {exact_match_score*100:.2f}%\")\n",
        "correct_count = sum(r['correct'] for r in detailed_results)\n",
        "incorrect_count = len(detailed_results) - correct_count\n",
        "print(f\"  Correct: {correct_count}\")\n",
        "print(f\"  Incorrect: {incorrect_count}\")\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}